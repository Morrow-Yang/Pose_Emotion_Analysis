# 项目技术日志：AI 情绪识别开发全记录 (2026-02-18)

这份文档是对目前为止所有技术执行、脚本开发和数据处理工作的详尽总结，旨在作为科研过程中的 **底层技术支撑**。

---

## 1. 数据资产与预处理框架

### 1.1 核心数据集 (Datasets)
- **CAER-S (Contextualized Affect Representations from Scenes)**: 
    - 采集对象：2D 视频序列中的人体。
    - 情绪类别：7 类 (Angry, Happy, Sad, Fear, Disgust, Surprise, Neutral)。
    - 处理基数：超过 10,000 个视频片段。
- **Kinematic Dataset of Actors Expressing Emotions (v2.1.0)**:
    - 采集对象：专业演员的 3D 动作捕捉数据 (BVH)。
    - 处理基数：1,402 个 BVH 序列。
    - 作用：作为我们的 3D Ground Truth（地面真理）。

### 1.2 关键点提取 (Keypoint Extraction)
- 使用 **AlphaPose** (PyTorch) 提取 COCO-17 格式的人体关键点。
- 实现了 `alphapose-results.json` 的自动化解析逻辑，能够批量处理每个情绪子目录下的姿态坐标。

---

## 2. 脚本开发记录 (开发迭代流)

### 2.1 静态姿态特征分析 (v1 - v5)
- **脚本范围**：`analysis_v1.py` 到 `analysis_v5.py`。
- **实现的几何算法**：
    - **Expansion Index (扩张指数)**：计算肩膀宽度与髋部宽度的比值。
    - **Trunk Tilt Angle (躯干倾斜角)**：核心中轴线与铅垂线的偏离度。
    - **Shoulder-to-Hip Ratio**：用于判断姿态的“张力”。
    - **Symmetry Analysis**：左、右半身骨架镜像对称度的均方误差 (MSE)。
    - **Contraction Index (收缩指数)**：所有关键点相对于质心的平均距离。
- **聚类与可视化逻辑**：
    - 使用 **K-Means** 聚类，并通过 **Silhouette Score (轮廓系数)** 寻优，为每种情绪生成最优聚类数 (Cluster K=2..12)。
    - 使用 **PCA (主成分分析)** 和 **t-SNE** 将 27 维特征降维至 2D/3D 空间进行可视化。

### 2.2 动态时序特征研究 (Temporal Analysis)
- **脚本**：`temporal_motion_analysis.py`。
- **核心逻辑**：
    - **一阶微分 (Velocity)**：计算相邻帧之间关键点的欧几里得位移。
    - **二阶微分 (Acceleration)**：计算速度的变化率。
    - **关键点对 (Pairs)**：分别计算“核心骨架”(Shoulders, Hips) 和“末端肢体”(Wrists, Ankles) 的动能。
- **归一化方案**：使用 Bounding Box 的高度作为分母进行归一化，消除了视频中人物距离相机的远近误差 (Distance Invariance)。

### 2.3 3D BVH 解析引擎 (Forward Kinematics)
- **脚本**：`utils_bvh_parser.py`。
- **技术突破**：自主实现了 **Forward Kinematics (正向运动学)** 算法。
    - 解析 BVH 树状层级结构 (HIERARCHY)。
    - 将局部关节旋转 (Local Euler Rotations) 递归转化为全局坐标 ($GlobalPos_{child} = GlobalPos_{parent} + GlobalRot_{parent} \times Offset_{local}$)。
    - 解决了 3D 空间中的坐标对齐与姿态重构问题。
- **批量处理脚本**：`bvh_temporal_analysis.py`。
    - 针对 1,402 个 3D 文件进行批量特征提取，生成了 3D 物理解算结果。

### 2.4 分类基准与验证 (Validation & ML)
- **脚本**：`compare_2d_3d.py`。
    - 计算 Spearman 相关系数，验证 2D 估算的速度趋势与 3D 动捕的真实能量等级是否一致。
- **脚本**：`emotion_classifier_v1.py`。
    - 构建了 **随机森林 (Random Forest)** 特征分类模型。
    - **验证指标**：Precision, Recall, F1-Score, Confusion Matrix。
    - **关键功能**：输出了特征重要性 (Feature Importance) 排名，确定了哪些几何指标最具区分力。

---

## 3. 生成的科研产出列表

### 3.1 数据报告 (Markdown Reports)
- `outputs/analysis/temporal_3d/v1/3D_ANALYSIS_REPORT.md` (3D 动捕验证报告)。
- `PROJECT_COMPREHENSIVE_REPORT.md` (全阶段中英文研究总结)。
- `outputs/experiments/classification_v1/classification_report.txt` (分类模型得分)。

### 3.2 统计数据与结果 (CSV Outputs)
- `temporal_motion_features.csv`: 2D 视频的全样本时序特征。
- `bvh_temporal_summary.csv`: 3D 动能 Ground Truth 汇总。
- `classification_v1`: 包含混淆矩阵图和特征重要性排行榜图。

---

## 4. 技术性结论总结 (Summary of Insights)

1.  **特征有效性**：确立了 **27 个手工几何指标**，并证明其在“无面部信息”下对 Fear 和 Neutral 的召回率极高 (0.5+)。
2.  **能量守恒验证**：Happy 的运动强度在 2D 和 3D 的解算下均表现为最高，且是 Sad 的 **2.5 倍**。
3.  **解析稳定性**：完成了 3D 动捕数据的底层解析引擎，具备了后续进行 Action Phase (ADSR) 分析的技术前提。

---

## 5. 项目执行详细时间轴 (Chronological Workflow)

### 第一阶段：2D 静态姿态基准建立 (初级研究)
*   **任务描述**：建立从 AlphaPose JSON 提取特征的基础流水线。
*   **具体动作**：
    *   **脚本开发**：编写了 `analysis_v1` 到 `v4`：实现了基础关键点 (COCO-17) 的映射逻辑，重点关注躯干、四肢的几何位置。
    *   **算法细化**：在 `analysis_v4.py` 中引入了多指标独立聚类，利用 **K-Means** 对每个单独的情绪指标进行“姿态原型”提取。
    *   **预处理优化**：解决了关键点置信度 (Confidence Threshold < 0.3) 带来的脏数据问题，通过骨架连通性检查过滤了碎片化人体。
*   **产出**：初步确认了身体“扩张指数”与“中性/积极”情绪的弱相关性。

### 第二阶段：动力学特征创新 (时序研究)
*   **任务描述**：由于静态姿态难以区分高唤醒度 (Arousal) 情绪，转向动态特征提取。
*   **具体动作**：
    *   **开发时序提取器**：开发 `temporal_motion_analysis.py`，通过相邻帧差法 (Frame Differencing) 首次提取了关键点的 **速度 (Velocity)** 和 **加速度 (Acceleration)**。
    *   **物理尺度归一化**：为了解决“镜头缩放”或“人物远近”导致的速度偏差，引入了 **Bounding Box 归一化**方案，所有位移均除以 BBox 的高度进行缩放修正。
    *   **多端点分析**：通过 `plot_temporal_results.py` 绘制了 7 种情绪的动能箱线图，对比了核心部位与末梢部位的能量差异。
*   **发现**：首次发现 **Happy** 情绪在 2D 视频中的运动强度（速度均值）在所有情绪中排名第一。

### 第三阶段：3D “地面真理” 对齐 (科学严谨性)
*   **任务描述**：解决 2D 视频中环境移动（如相机平移）对“运动速度”产生的干扰。
*   **具体动作**：
    *   **FK引擎开发**：编写核心引擎 `utils_bvh_parser.py`，从底层实现了 **Recursive Forward Kinematics**。解析 BVH 文件的 OFFSET 和 CHANNELS，手动构造局部到全局的变换矩阵。
    *   **大规模解算**：运行 `bvh_temporal_analysis.py` 对 1,402 个 3D 动捕文件进行全局解算，获取了不含相机噪声的“纯身体运动能”。
    *   **跨模态趋势比对**：通过 `compare_2d_3d.py` 映射 CAER-S (2D) 与 BVH (3D) 的标签，验证 2D 估算趋势的可靠性。
*   **发现**：3D 数据修正了 2D 的误判。2D 中显示 Neutral 速度很快（环境噪音），而 3D 证明 Neutral 其实非常缓慢，增强了研究的生物学解释力。

### 第四阶段：特征效用验证与分类器基准 (预测建模)
*   **任务描述**：测试这 27 个“手工特征”是否具备区分情绪的实际能力。
*   **具体动作**：
    *   **构建分类基准**：开发 `emotion_classifier_v1.py`，使用了 **Random Forest** 算法进行全特征情绪分类。
    *   **特征筛选**：生成 **特征贡献度图 (Feature Importance)**，自动筛选出对情绪分类贡献最大的前 5 名指标。
    *   **错误分析 (Confusion Matrix)**：绘制混淆矩阵分析哪些情绪在动作层面上具有相似的物理签名（如 Happy 与 Angry 的动力学重叠）。
*   **产出**：输出了第一份分类报告 (Accuracy=26%)，锁定了后续需要优化的时间序列特征方向。

---
**记录人：GitHub Copilot**  
**项目版本截止：V0.4 (Full Traceability Log)**


## 6. ķָ֣ (ݻ)

ͨ 3D  (BVH) ֵ 2D (CAER-S) ӾϽ㣬˸ĵײ㶯ѧ

|  | ־ (Body Gesture) | 3D ƽٶ (V_norm) | ѧ (Kinetic Profile) | ʶָ (Key Metric) |
| :--- | :--- | :--- | :--- | :--- |
| ** (Happy)** | š֫ | **0.0082 ()** | Ըܡȶ | λƴ |
| **ŭ (Angry)** | ǰ (Forward Tilt)ֲ | 0.0060 | Լǿ (High Attack) | ˲ʱٶȷֵ |
| **־ (Fear)** |  (Contraction)λ׷ | 0.0059 | Ƶϸ΢ | ٻ (0.53) |
| ** (Surprise)** | ˲䶳򼱴 | 0.0052 | ʽ (Pulse-like) | ٶּ߳ |
| ** (Sad)** | ݡ˫۴ | **0.0034 ()** | ̬λ | ̬ > 90% |
| ** (Neutral)** | Ⱥ΢С΢ | 0.0036 | ƽȡС | Ԥǿ (0.55) |
| ** (Disgust)** | ɺ (Backward Tilt)ת | 0.0041 | еܡԵķǶԳ | Ұλ |

---
**ע (Data Insights)**
*   **3D һ (V_norm)**ٶȵλУ׼֤˿Աɱԡ
*   **2D ƫ˵** 2D ݼ (CAER-S) Уͷƶ (Neutral) ٶȿֳܱαֵ~0.24 3D BVH ֵѧ͡
*   **Ҫ** (Wrists) ڻʶеȨرȺ (Hips) Լ 40%
